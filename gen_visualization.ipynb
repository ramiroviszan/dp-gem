{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('results/{exp_name}/gen.h5'.format(exp_name=exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 50, 2)             62        \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 1024)              4206592   \n_________________________________________________________________\nrepeat_vector_1 (RepeatVecto (None, 50, 1024)          0         \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 50, 1024)          8392704   \n_________________________________________________________________\ntime_distributed_2 (TimeDist (None, 50, 31)            31775     \n=================================================================\nTotal params: 12,631,133\nTrainable params: 12,631,133\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "31"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "embedding = model.layers[0].get_weights()[0]\n",
    "vocab_size = embedding.shape[0]\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "anomal = np.array([6, 7, 9, 11, 12, 13, 14, 18, 23, 26, 28]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7 0 0.9998777 False\n7 1 0.5913588 False\n7 2 -0.65500975 False\n7 3 -0.71910644 False\n7 4 -0.42512882 False\n7 5 0.75074494 False\n7 6 0.99492425 False\n7 7 0.99999994 True\n7 8 0.9980128 True\n7 9 -0.35247767 False\n7 10 0.7310178 True\n7 11 -0.6520323 False\n7 12 0.94377834 True\n7 13 -0.88051236 True\n7 14 -0.15278327 True\n7 15 -0.38700783 True\n7 16 0.5207794 False\n7 17 0.9623938 False\n7 18 0.87381065 False\n7 19 -0.91992354 True\n7 20 0.71103823 False\n7 21 0.99601007 False\n7 22 -0.6683843 False\n7 23 0.35667357 False\n7 24 -0.49170187 True\n7 25 0.53760535 False\n7 26 0.31799513 False\n7 27 0.07795444 True\n7 28 0.7714318 False\n7 29 0.99959964 True\n7 30 0.6306978 False\n8 0 0.99690557 False\n8 1 0.64099747 False\n8 2 -0.70132166 False\n8 3 -0.6738901 False\n8 4 -0.4813187 False\n8 5 0.79087865 False\n8 6 0.9866064 False\n8 7 0.9980128 True\n8 8 1.0000001 True\n8 9 -0.29280886 False\n8 10 0.77256227 True\n8 11 -0.60296106 False\n8 12 0.96273345 True\n8 13 -0.908632 True\n8 14 -0.21475238 True\n8 15 -0.32813644 True\n8 16 0.46595123 False\n8 17 0.94336355 False\n8 18 0.9027151 False\n8 19 -0.8933884 True\n8 20 0.66531783 False\n8 21 0.98840755 False\n8 22 -0.6201863 False\n8 23 0.41483286 False\n8 24 -0.43585575 True\n8 25 0.589669 False\n8 26 0.37710485 False\n8 27 0.14062026 True\n8 28 0.7298032 False\n8 29 0.99939615 True\n8 30 0.58054495 False\n10 0 0.7202598 False\n10 1 0.9825535 False\n10 2 -0.9944278 False\n10 3 -0.051508904 False\n10 4 -0.9284018 False\n10 5 0.9995686 False\n10 6 0.6586439 False\n10 7 0.7310178 True\n10 8 0.77256227 True\n10 9 0.38089737 False\n10 10 1.0000001 True\n10 11 0.04071094 False\n10 12 0.9154922 True\n10 13 -0.967124 True\n10 14 -0.7860347 True\n10 15 0.34627694 True\n10 16 -0.20182449 False\n10 17 0.518159 False\n10 18 0.9705791 False\n10 19 -0.40492934 True\n10 20 0.03997904 False\n10 21 0.6672071 False\n10 22 0.018948376 False\n10 23 0.8982138 False\n10 24 0.23473005 True\n10 25 0.9683614 False\n10 26 0.8793989 False\n10 27 0.73726803 True\n10 28 0.12973756 False\n10 29 0.7500323 True\n10 30 -0.06847826 False\n12 0 0.9384944 False\n12 1 0.8246934 False\n12 2 -0.86797637 False\n12 3 -0.4489578 False\n12 4 -0.70044535 False\n12 5 0.9269148 False\n12 6 0.90572286 False\n12 7 0.94377834 True\n12 8 0.96273345 True\n12 9 -0.023298234 False\n12 10 0.9154922 True\n12 11 -0.36473185 False\n12 12 1.0000001 True\n12 13 -0.98771065 True\n12 14 -0.47089154 True\n12 15 -0.060430497 True\n12 16 0.20928779 False\n12 17 0.81848204 False\n12 18 0.98543304 False\n12 19 -0.7385846 True\n12 20 0.43861473 False\n12 21 0.91051173 False\n12 22 -0.3849166 False\n12 23 0.6454573 False\n12 24 -0.17620137 True\n12 25 0.78612345 False\n12 26 0.61353636 False\n12 27 0.40314478 True\n12 28 0.51770926 False\n12 29 0.95275414 True\n12 30 0.33870015 False\n13 0 -0.8729934 False\n13 1 -0.90295506 False\n13 2 0.93492585 False\n13 3 0.30378357 False\n13 4 0.8033852 False\n13 5 -0.9741762 False\n13 6 -0.82834375 False\n13 7 -0.88051236 True\n13 8 -0.908632 True\n13 9 -0.13323952 False\n13 10 -0.967124 True\n13 11 0.2147223 False\n13 12 -0.98771065 True\n13 13 1.0 True\n13 14 0.6029856 True\n13 15 -0.096320376 True\n13 16 -0.053883195 False\n13 17 -0.7186275 False\n13 18 -0.9999026 False\n13 19 0.6241406 True\n13 20 -0.29276702 False\n13 21 -0.8346971 False\n13 22 0.23593459 False\n13 23 -0.7569017 False\n13 24 0.02018746 True\n13 25 -0.8730629 False\n13 26 -0.7294164 False\n13 27 -0.5412205 True\n13 28 -0.37762865 False\n13 29 -0.8935723 True\n13 30 -0.18748172 False\n14 0 -0.1373131 False\n14 1 -0.88729084 False\n14 2 0.8468239 False\n14 3 -0.5768741 False\n14 4 0.959459 False\n14 5 -0.7675379 False\n14 6 -0.052562535 False\n14 7 -0.15278327 True\n14 8 -0.21475238 True\n14 9 -0.8709808 False\n14 10 -0.7860347 True\n14 11 -0.6496703 False\n14 12 -0.47089154 True\n14 13 0.6029856 True\n14 14 1.0 True\n14 15 -0.85212266 True\n14 16 0.76410246 False\n14 17 0.12143096 False\n14 18 -0.61406106 False\n14 19 -0.24694547 True\n14 20 0.5862634 False\n14 21 -0.06398082 False\n14 22 -0.6329656 False\n14 23 -0.9777548 False\n14 24 -0.78541684 True\n14 25 -0.91543436 False\n14 26 -0.9855458 False\n14 27 -0.9971625 True\n14 28 0.5109797 False\n14 29 -0.18068469 True\n14 30 0.6705577 False\n15 0 -0.40137726 False\n15 1 0.51470983 False\n15 2 -0.4432459 False\n15 3 0.9190507 False\n15 4 -0.6700735 False\n15 5 0.31857222 False\n15 6 -0.47782898 False\n15 7 -0.38700783 True\n15 8 -0.32813644 True\n15 9 0.9993094 False\n15 10 0.34627694 True\n15 11 0.95145196 False\n15 12 -0.060430497 True\n15 13 -0.096320376 True\n15 14 -0.85212266 True\n15 15 1.0 True\n15 16 -0.9887144 False\n15 17 -0.6229434 False\n15 18 0.110203065 False\n15 19 0.7175617 True\n15 20 -0.92353857 False\n15 21 -0.46775037 False\n15 22 0.94452536 False\n15 23 0.72339517 False\n15 24 0.99320316 True\n15 25 0.56943417 False\n15 26 0.751147 False\n15 27 0.88910156 True\n15 28 -0.88527846 False\n15 29 -0.36076304 True\n15 30 -0.95964265 False\n19 0 -0.9259416 False\n19 1 -0.2278139 False\n19 2 0.30628186 False\n19 3 0.9339916 False\n19 4 0.036185473 False\n19 5 -0.4316113 False\n19 6 -0.9547097 False\n19 7 -0.91992354 True\n19 8 -0.8933884 True\n19 9 0.6911855 False\n19 10 -0.40492934 True\n19 11 0.897105 False\n19 12 -0.7385846 True\n19 13 0.6241406 True\n19 14 -0.24694547 True\n19 15 0.7175617 True\n19 16 -0.8138076 False\n19 17 -0.99184525 False\n19 18 -0.6131751 False\n19 19 0.99999994 True\n19 20 -0.9298057 False\n19 21 -0.9512441 False\n19 22 0.90651107 False\n19 23 0.038196743 False\n19 24 0.79375255 True\n19 25 -0.16394034 False\n19 26 0.07921365 False\n19 27 0.31919247 True\n19 28 -0.9591548 False\n19 29-0.908461 True\n19 30 -0.8844728 False\n24 0 -0.50525624 False\n24 1 0.41141903 False\n24 2 -0.33589727 False\n24 3 0.95867956 False\n24 4 -0.57912016 False\n24 5 0.20607689 False\n24 6 -0.5768282 False\n24 7 -0.49170187 True\n24 8 -0.43585575 True\n24 9 0.9881922 False\n24 10 0.23473005 True\n24 11 0.98081094 False\n24 12 -0.17620137 True\n24 13 0.02018746 True\n24 14 -0.78541684 True\n24 15 0.99320316 True\n24 16 -0.99943155 False\n24 17 -0.7097607 False\n24 18 -0.00623136 False\n24 19 0.79375255 True\n24 20 -0.9618993 False\n24 21 -0.5674475 False\n24 22 0.9763339 False\n24 23 0.63811564 False\n24 24 1.0 True\n24 25 0.46988338 False\n24 26 0.6692055 False\n24 27 0.82978356 True\n24 28 -0.93339187 False\n24 29 -0.46686703 True\n24 30 -0.9858527 False\n27 0 0.062357426 False\n27 1 0.85005367 False\n27 2 -0.80438197 False\n27 3 0.6367277 False\n27 4 -0.9355192 False\n27 5 0.7171058 False\n27 6 -0.022761703 False\n27 7 0.07795444 True\n27 8 0.14062026 True\n27 9 0.90549535 False\n27 10 0.73726803 True\n27 11 0.70505524 False\n27 12 0.40314478 True\n27 13 -0.5412205 True\n27 14 -0.9971625 True\n27 15 0.88910156 True\n27 16 -0.81049645 False\n27 17 -0.1958085 False\n27 18 0.55290395 False\n27 19 0.31919247 True\n27 20 -0.6455851 False\n27 21 -0.011325598 False\n27 22 0.6894492 False\n27 23 0.9591905 False\n27 24 0.82978356 True\n27 25 0.8825394 False\n27 26 0.96999633 False\n27 27 1.0 True\n27 28 -0.5742392 False\n27 29 0.10613182 True\n27 30 -0.7245014 False\n29 0 0.9990351 False\n29 1 0.6139391 False\n29 2 -0.6761276 False\n29 3 -0.69915664 False\n29 4 -0.45056912 False\n29 5 0.7691357 False\n29 6 0.9916788 False\n29 7 0.99959964 True\n29 8 0.99939615 True\n29 9 -0.32585782 False\n29 10 0.7500323 True\n29 11 -0.6303185 False\n29 12 0.95275414 True\n29 13 -0.8935723 True\n29 14 -0.18068469 True\n29 15 -0.36076304 True\n29 16 0.4964159 False\n29 17 0.9543221 False\n29 18 0.88721967 False\n29 19 -0.908461 True\n29 20 0.6908581 False\n29 21 0.99308634 False\n29 22 -0.64707065 False\n29 23 0.38296452 False\n29 24 -0.46686703 True\n29 25 0.5612482 False\n29 26 0.34469384 False\n29 27 0.10613182 True\n29 28 0.75311875 False\n29 29 1.0000001 True\n29 30 0.6084878 False\n"
    }
   ],
   "source": [
    "for i in anomal:\n",
    "    for j in range(0, vocab_size):\n",
    "        print(i, j, cosine_similarity(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))[0][0], j in anomal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n1 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n2 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n3 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n4 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n5 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n6 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n7 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n8 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n9 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n10 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n11 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n12 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n13 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n14 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n15 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n16 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n17 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n18 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n19 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n20 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n21 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n22 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n23 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n24 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n25 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n26 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n27 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n28 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n29 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n30 [14  1  2  3  4] [0.26388812 0.26388812 0.26388812 0.26388812 0.26388812] [True, False, False, False, False]\n-----------------\n"
    }
   ],
   "source": [
    "for i in range(0, vocab_size):\n",
    "    all_cos = []\n",
    "    for j in range(0, vocab_size):\n",
    "        cos = cosine_similarity(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))[0][0]\n",
    "        all_cos.append(cos1)\n",
    "\n",
    "    all_cos = np.array(all_cos)\n",
    "\n",
    "    top_k_cos = np.argsort(all_cos)[-6:][::-1][1:]\n",
    "\n",
    "    print(i, top_k_cos, all_cos[top_k_cos], [x in anomal for x in top_k_cos])\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-60-e2992d0e0620>, line 7)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-60-e2992d0e0620>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    plt.annotate(symbol, (x[i], y[i]), color=['red' for symbol in txt if symbols in anomal else 'blue'])\u001b[0m\n\u001b[1;37m                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = [symbol[0]  for symbol in embedding]\n",
    "y = [symbol[1]  for symbol in embedding]\n",
    "symbols = list(range(vocab_size))\n",
    "plt.scatter(x, y, ) \n",
    "\n",
    "for i, symbol in enumerate(symbols):\n",
    "    color = 'red' if symbol in anomal else 'black'\n",
    "    plt.annotate(symbol, (x[i], y[i]), color=['red' for symbol in txt if symbol in anomal else 'blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('keras': conda)",
   "language": "python",
   "name": "python37364bitkerascondaa4014ad699b9423aac6666553c6a2934"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}