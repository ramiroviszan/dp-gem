{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp_3_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING: Logging before flag parsing goes to stderr.\nW0703 16:51:21.171715 22760 deprecation.py:506] From C:\\Users\\Ramiro\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nW0703 16:51:21.196689 22760 deprecation.py:506] From C:\\Users\\Ramiro\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nW0703 16:51:24.749900 22760 deprecation.py:323] From C:\\Users\\Ramiro\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n"
    }
   ],
   "source": [
    "model = load_model('results/{exp_name}/gen.h5'.format(exp_name=exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 46, 8)             248       \n_________________________________________________________________\nlambda (Lambda)              (None, 8)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               4608      \n_________________________________________________________________\ndense_1 (Dense)              (None, 265)               135945    \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               34048     \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 174,978\nTrainable params: 174,978\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "anomal = np.array([6, 7, 9, 11, 12, 13, 14, 18, 23, 26, 28]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7 0 0.48234162 False\n7 1 -0.62084806 False\n7 2 -0.36072463 False\n7 3 -0.19631049 False\n7 4 -0.15576626 False\n7 5 -0.61790764 False\n7 6 -0.39579764 False\n7 7 1.0000002 True\n7 8 0.7658654 True\n7 9 -0.5973449 False\n7 10 0.7620117 True\n7 11 -0.65455955 False\n7 12 0.43614087 True\n7 13 0.39037603 True\n7 14 0.45203418 True\n7 15 0.22653677 True\n7 16 -0.06144531 False\n7 17 -0.52545506 False\n7 18 -0.36623442 False\n7 19 -0.06617364 True\n7 20 0.1018621 False\n7 21 -0.38459718 False\n7 22 -0.027608048 False\n7 23 -0.66588974 False\n7 24 -0.47164044 True\n7 25 0.2392138 False\n7 26 -0.35872293 False\n7 27 0.18524814 True\n7 28 0.17930469 False\n7 29 0.006827742 True\n7 30 0.25208765 False\n8 0 0.5321672 False\n8 1 -0.75527084 False\n8 2 -0.1308046 False\n8 3 -0.012748182 False\n8 4 0.0019973367 False\n8 5 -0.6260704 False\n8 6 -0.3473407 False\n8 7 0.7658654 True\n8 8 1.0 True\n8 9 -0.6678062 False\n8 10 0.7549696 True\n8 11 -0.8811557 False\n8 12 0.29567298 True\n8 13 0.48336554 True\n8 14 0.614429 True\n8 15 -0.025576383 True\n8 16 0.051018506 False\n8 17 -0.49041045 False\n8 18 -0.13793029 False\n8 19 -0.5298254 True\n8 20 -0.0032087788 False\n8 21 -0.61080724 False\n8 22 -0.057505727 False\n8 23 -0.94241285 False\n8 24 0.092545286 True\n8 25 0.56129754 False\n8 26 -0.55438846 False\n8 27 0.05315306 True\n8 28 -0.034219168 False\n8 29 -0.32287252 True\n8 30 -0.13707705 False\n10 0 0.37475273 False\n10 1 -0.66934276 False\n10 2 -0.46249467 False\n10 3 -0.19191143 False\n10 4 -0.26503322 False\n10 5 -0.55420935 False\n10 6 -0.32454333 False\n10 7 0.7620117 True\n10 8 0.7549696 True\n10 9 -0.5848831 False\n10 10 0.9999998 True\n10 11 -0.85236967 False\n10 12 0.7423792 True\n10 13 0.60552335 True\n10 14 0.8303634 True\n10 15 0.019167304 True\n10 16 -0.05867766 False\n10 17 -0.10868946 False\n10 18 0.049111843 False\n10 19 -0.3444913 True\n10 20 0.1731882 False\n10 21 -0.7038591 False\n10 22 -0.42414057 False\n10 23 -0.7895364 False\n10 24 -0.23149058 True\n10 25 0.5807375 False\n10 26 -0.47698906 False\n10 27 0.27258253 True\n10 28 0.19170347 False\n10 29 0.04810269 True\n10 30 -0.07386908 False\n12 0 -0.23920856 False\n12 1 -0.60064185 False\n12 2 -0.86914873 False\n12 3 -0.6806969 False\n12 4 -0.68865585 False\n12 5 0.05309558 False\n12 6 -0.5676609 False\n12 7 0.43614087 True\n12 8 0.29567298 True\n12 9 -0.107443586 False\n12 10 0.7423792 True\n12 11 -0.37640893 False\n12 12 1.0 True\n12 13 0.78051007 True\n12 14 0.70176715 True\n12 15 0.35079268 True\n12 16 -0.5602228 False\n12 17 -0.030746326 False\n12 18 -0.05132863 False\n12 19 -0.22566971 True\n12 20 0.6623672 False\n12 21 -0.3276435 False\n12 22 -0.20712328 False\n12 23 -0.3189599 False\n12 24 -0.23204914 True\n12 25 0.30166292 False\n12 26 -0.0854163 False\n12 27 0.7167547 True\n12 28 0.6530713 False\n12 29 0.6143341 True\n12 30 0.29071274 False\n13 0 -0.36880153 False\n13 1 -0.85050356 False\n13 2 -0.8344337 False\n13 3 -0.7459496 False\n13 4 -0.7344959 False\n13 5 0.22681232 False\n13 6 -0.72653365 False\n13 7 0.39037603 True\n13 8 0.48336554 True\n13 9 0.14302427 False\n13 10 0.60552335 True\n13 11 -0.32445416 False\n13 12 0.78051007 True\n13 13 1.0 True\n13 14 0.5462787 True\n13 15 0.6308931 True\n13 16 -0.6288364 False\n13 17 -0.30784672 False\n13 18 -0.20382708 False\n13 19 -0.5849544 True\n13 20 0.8276142 False\n13 21 -0.18635052 False\n13 22 0.25097924 False\n13 23 -0.37089133 False\n13 24 -0.004345417 True\n13 25 0.2563953 False\n13 26 0.06188248 False\n13 27 0.8644155 True\n13 28 0.766887 False\n13 29 0.47610396 True\n13 30 0.3522041 False\n14 0 0.266252 False\n14 1 -0.63227284 False\n14 2 -0.43359035 False\n14 3 -0.2295104 False\n14 4 -0.3555569 False\n14 5 -0.45939717 False\n14 6 -0.45719576 False\n14 7 0.45203418 True\n14 8 0.614429 True\n14 9 -0.57998806 False\n14 10 0.8303634 True\n14 11 -0.7420771 False\n14 12 0.70176715 True\n14 13 0.5462787 True\n14 14 1.0 True\n14 15 -0.08613685 True\n14 16 0.06711341 False\n14 17 0.19580299 False\n14 18 0.0011932552 False\n14 19 -0.46226218 True\n14 20 0.16064249 False\n14 21 -0.6648564 False\n14 22 -0.35481435 False\n14 23 -0.7770962 False\n14 24 0.24893582 True\n14 25 0.60394573 False\n14 26 -0.70658636 False\n14 27 0.22323512 True\n14 28 0.19361296 False\n14 29 -0.09939735 True\n14 30 -0.28400654 False\n15 0 -0.5558866 False\n15 1 -0.45901254 False\n15 2 -0.75120205 False\n15 3 -0.82340133 False\n15 4 -0.78033507 False\n15 5 0.4860511 False\n15 6 -0.65227413 False\n15 7 0.22653677 True\n15 8 -0.025576383 True\n15 9 0.56065506 False\n15 10 0.019167304 True\n15 11 0.36967194 False\n15 12 0.35079268 True\n15 13 0.6308931 True\n15 14 -0.08613685 True\n15 15 1.0000001 True\n15 16 -0.6012783 False\n15 17 -0.38067734 False\n15 18 -0.64275557 False\n15 19 -0.18305296 True\n15 20 0.83322525 False\n15 21 0.4770772 False\n15 22 0.75001 False\n15 23 0.24425822 False\n15 24 -0.3001721 True\n15 25 -0.310019 False\n15 26 0.42646205 False\n15 27 0.8469684 True\n15 28 0.8865981 False\n15 29 0.58869326 True\n15 30 0.6800351 False\n19 0 0.028990552 False\n19 1 0.559715 False\n19 2 0.22209018 False\n19 3 0.25470644 False\n19 4 0.39925033 False\n19 5 0.028542534 False\n19 6 0.215018 False\n19 7 -0.06617364 True\n19 8 -0.5298254 True\n19 9 0.087947346 False\n19 10 -0.3444913 True\n19 11 0.36098272 False\n19 12 -0.22566971 True\n19 13 -0.5849544 True\n19 14 -0.46226218 True\n19 15 -0.18305296 True\n19 16 0.13764247 False\n19 17 0.20865282 False\n19 18 0.0712997 False\n19 19 0.9999999 True\n19 20 -0.31504196 False\n19 21 0.37186006 False\n19 22 -0.15517934 False\n19 23 0.48337412 False\n19 24 -0.42946354 True\n19 25 -0.6935151 False\n19 26 0.36784142 False\n19 27 -0.33156297 True\n19 28 -0.23995826 False\n19 29 0.1334126 True\n19 30 0.43053484 False\n24 0 -0.05726383 False\n24 1 -0.11410245 False\n24 2 0.2608662 False\n24 3 0.13505004 False\n24 4 0.11296475 False\n24 5 0.090842344 False\n24 6 -0.1751332 False\n24 7 -0.47164044 True\n24 8 0.092545286 True\n24 9 -0.055696234 False\n24 10 -0.23149058 True\n24 11 -0.07445651 False\n24 12 -0.23204914 True\n24 13 -0.004345417 True\n24 14 0.24893582 True\n24 15 -0.3001721 True\n24 16 0.22617662 False\n24 17 0.25615168 False\n24 18 0.08300674 False\n24 19 -0.42946354 True\n24 20 -0.11833532 False\n24 21 -0.085889414 False\n24 22 0.151555 False\n24 23 -0.23665309 False\n24 24 1.0 True\n24 25 0.16798297 False\n24 26 -0.35824114 False\n24 27 -0.19008197 True\n24 28 -0.19875869 False\n24 29 -0.47352034 True\n24 30 -0.42818686 False\n27 0 -0.6936959 False\n27 1 -0.6144598 False\n27 2 -0.9298352 False\n27 3 -0.9155612 False\n27 4 -0.90340984 False\n27 5 0.5638117 False\n27 6 -0.70866954 False\n27 7 0.18524814 True\n27 8 0.05315306 True\n27 9 0.47230303 False\n27 10 0.27258253 True\n27 11 0.14016068 False\n27 12 0.7167547 True\n27 13 0.8644155 True\n27 14 0.22323512 True\n27 15 0.8469684 True\n27 16 -0.80098796 False\n27 17 -0.27417946 False\n27 18 -0.32456887 False\n27 19 -0.33156297 True\n27 20 0.9906945 False\n27 21 0.15828474 False\n27 22 0.4487119 False\n27 23 0.098359786 False\n27 24 -0.19008197 True\n27 25 -0.07984434 False\n27 26 0.35843194 False\n27 27 0.9999998 True\n27 28 0.9691389 False\n27 290.75435156 True\n27 30 0.586899 False\n29 0 -0.62865686 False\n29 1 -0.073987976 False\n29 2 -0.75893456 False\n29 3 -0.78030014 False\n29 4 -0.60697997 False\n29 5 0.6168122 False\n29 6 -0.30685234 False\n29 7 0.006827742 True\n29 8 -0.32287252 True\n29 9 0.51402795 False\n29 10 0.04810269 True\n29 11 0.3726609 False\n29 12 0.6143341 True\n29 13 0.47610396 True\n29 14 -0.09939735 True\n29 15 0.58869326 True\n29 16 -0.88663566 False\n29 17 -0.22436036 False\n29 18 -0.18830103 False\n29 19 0.1334126 True\n29 20 0.7540183 False\n29 21 0.39610556 False\n29 22 0.13815433 False\n29 23 0.4399308 False\n29 24 -0.47352034 True\n29 25 -0.16344538 False\n29 26 0.6218071 False\n29 27 0.75435156 True\n29 28 0.69630486 False\n29 29 1.0000002 True\n29 30 0.6702754 False\n"
    }
   ],
   "source": [
    "for i in anomal:\n",
    "    for j in range(0, 31):\n",
    "        print(i, j, cosine_similarity(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))[0][0], j in anomal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7 [ 8 10  0 14 12] [0.7658654  0.7620117  0.48234162 0.45203418 0.43614087] [True, True, False, True, True]\n-----------------\n8 [ 7 10 14 25  0] [0.7658654  0.7549696  0.614429   0.56129754 0.5321672 ] [True, True, True, False, False]\n-----------------\n10 [14  7  8 12 13] [0.8303634  0.7620117  0.7549696  0.7423792  0.60552335] [True, True, True, True, True]\n-----------------\n12 [13 10 27 14 20] [0.78051007 0.7423792  0.7167547  0.70176715 0.6623672 ] [True, True, True, True, False]\n-----------------\n13 [27 20 12 28 15] [0.8644155  0.8276142  0.78051007 0.766887   0.6308931 ] [True, False, True, False, True]\n-----------------\n14 [10 12  8 25 13] [0.8303634  0.70176715 0.614429   0.60394573 0.5462787 ] [True, True, True, False, True]\n-----------------\n15 [28 27 20 22 30] [0.8865981  0.8469684  0.83322525 0.75001    0.6800351 ] [False, True, False, False, False]\n-----------------\n19 [ 1 23 30  4 21] [0.559715   0.48337412 0.43053484 0.39925033 0.37186006] [False, False, False, False, False]\n-----------------\n24 [ 2 17 14 16 25] [0.2608662  0.25615168 0.24893582 0.22617662 0.16798297] [False, False, True, False, False]\n-----------------\n27 [20 28 13 15 29] [0.9906945  0.9691389  0.8644155  0.8469684  0.75435156] [False, False, True, True, True]\n-----------------\n29 [27 20 28 30 26] [0.75435156 0.7540183  0.69630486 0.6702754  0.6218071 ] [True, False, False, False, False]\n-----------------\n"
    }
   ],
   "source": [
    "for i in anomal:\n",
    "    all_cos1 = []\n",
    "\n",
    "    for j in range(0, 31):\n",
    "        cos1 = cosine_similarity(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))[0][0]\n",
    "        #print(i, j, cos1, cos2, cos1 - cos2, j in anomal)\n",
    "        all_cos1.append(cos1)\n",
    "\n",
    "\n",
    "    all_cos1 = np.array(all_cos1)\n",
    "\n",
    "    top_k_cos1 = np.argsort(all_cos1)[-6:][::-1][1:]\n",
    "\n",
    "    print(i, top_k_cos1, all_cos1[top_k_cos1], [x in anomal for x in top_k_cos1])\n",
    "\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('results/{exp_name}/deeplog_dp_gen_emb.h5'.format(exp_name='exp_1'))\n",
    "#model2 = load_model('results/{exp_name}/deeplog_dp_gen_emb.h5'.format(exp_name='exp_3'))\n",
    "model3 = load_model('results/{exp_name}/deeplog_dp_gen_emb.h5'.format(exp_name='exp_3'))\n",
    "embedding1 = model1.layers[0].get_weights()[0]\n",
    "embedding2 = model2.layers[0].get_weights()[0]\n",
    "embedding3 = model3.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6 [ 9  2  4 24] [0.53997374 0.5071637  0.49635866 0.4929083 ] [True, False, False, False]\n6 [12 19 27 26] [0.9076451  0.89326286 0.86851096 0.86789733] [True, False, False, True]\n6 [19 14 26  9] [0.8691887  0.86857164 0.843271   0.84148085] [False, True, True, True]\n-----------------\n7 [19  0 13 27] [0.92187214 0.7747382  0.6619732  0.64759254] [False, False, True, False]\n7 [19 27 26  6] [0.88320935 0.88095725 0.8514683  0.8320881 ] [False, False, True, True]\n7 [24 26 27 19] [0.93224996 0.89075387 0.8781583  0.8672353 ] [False, True, False, False]\n-----------------\n9 [16  0 24  6] [0.76236904 0.6599049  0.5875026  0.53997374] [False, False, False, True]\n9 [26 27 19 12] [0.88445497 0.8723365  0.8695332  0.85483104] [True, False, False, True]\n9 [26 27 19 12] [0.96490896 0.95519483 0.9365911  0.9340502 ] [True, False, False, True]\n-----------------\n11 [18  8 17 21] [0.98410904 0.94173414 0.81314594 0.7427062 ] [True, False, False, False]\n11 [19 12  6 26] [0.7687711  0.75770533 0.75708115 0.7440629 ] [False, True, True, True]\n11 [28 20  9 15] [0.86340415 0.86094624 0.82558954 0.82144386] [True, False, True, False]\n-----------------\n12 [ 5 15 17 21] [0.821472  0.8059784 0.6500386 0.5967867] [False, False, False, False]\n12 [26 19 27  6] [0.9573209  0.955487   0.94857556 0.9076451 ] [True, False, False, True]\n12 [27 19 26 14] [0.99550116 0.988664   0.9820833  0.96006525] [False, False, True, True]\n-----------------\n13 [22 23  7  1] [0.7645155  0.70149803 0.6619732  0.55250186] [False, True, True, False]\n13 [ 6 12 19 26] [0.8256214  0.81893724 0.77818596 0.7709583 ] [True, True, False, True]\n13 [20  7 24 27] [0.8072201  0.7928107  0.7837409  0.75964653] [False, True, False, False]\n-----------------\n14 [27 26 21 20] [0.8688567  0.76406026 0.7392428  0.73389006] [False, True, False, False]\n14 [19 27 26 12] [0.91859484 0.916816   0.9075377  0.9040234 ] [False, False, True, True]\n14 [19 26 27 12] [0.98183656 0.9652647  0.9611256  0.96006525] [False, True, False, True]\n-----------------\n18 [11  8 17  4] [0.98410904 0.8945638  0.819018   0.63561904] [True, False, False, False]\n18 [ 3 25  1 22] [0.38379404 0.3310241  0.31953797 0.2826905 ] [False, False, False, False]\n18 [ 2 23 21 13] [0.6258721  0.59442997 0.48804784 0.3464012 ] [False, True, False, True]\n-----------------\n23 [ 1  3  2 13] [0.9460249  0.8216605  0.71762204 0.70149803] [False, False, False, True]\n23 [25  4  2 20] [0.2811337  0.27744097 0.26821032 0.15827505] [False, False, False, False]\n23 [11  9 26 15] [0.73788524 0.6909395  0.63105214 0.61916554] [True, True, True, False]\n-----------------\n26 [25 20 21 14] [0.94083947 0.85420996 0.77407414 0.76406026] [False, False, False, True]\n26 [19 27 12 28] [0.98464286 0.9845277  0.9573209  0.92618847] [False, False, True, True]\n26 [27 19 12 14] [0.9927236 0.988269  0.9820833 0.9652647] [False, False, True, True]\n-----------------\n28 [25 20 19 26] [0.7973218  0.719823   0.596334   0.55942965] [False, False, False, True]\n28 [26 19 27 12] [0.92618847 0.92237675 0.90855455 0.8943893 ] [True, False, False, True]\n28 [14 26 24  9] [0.9518235  0.9405098  0.9345333  0.93152285] [True, True, False, True]\n-----------------\n"
    }
   ],
   "source": [
    "for i in anomal:\n",
    "    all_cos1 = []\n",
    "    all_cos2 = []\n",
    "    all_cos3 = []\n",
    "    for j in range(0, 29):\n",
    "        cos1 = cosine_similarity(embedding1[i].reshape(1, -1), embedding1[j].reshape(1, -1))[0][0]\n",
    "        cos2 = cosine_similarity(embedding2[i].reshape(1, -1), embedding2[j].reshape(1, -1))[0][0]\n",
    "        cos3 = cosine_similarity(embedding3[i].reshape(1, -1), embedding3[j].reshape(1, -1))[0][0]\n",
    "\n",
    "        #print(i, j, cos1, cos2, cos1 - cos2, j in anomal)\n",
    "        all_cos1.append(cos1)\n",
    "        all_cos2.append(cos2)\n",
    "        all_cos3.append(cos3)\n",
    "\n",
    "    all_cos1 = np.array(all_cos1)\n",
    "    all_cos2 = np.array(all_cos2)\n",
    "    all_cos3 = np.array(all_cos3)\n",
    "\n",
    "    top_k_cos1 = np.argsort(all_cos1)[-5:][::-1][1:]\n",
    "    top_k_cos2 = np.argsort(all_cos2)[-5:][::-1][1:]\n",
    "    top_k_cos3 = np.argsort(all_cos3)[-5:][::-1][1:]\n",
    "\n",
    "\n",
    "    print(i, top_k_cos1, all_cos1[top_k_cos1], [x in anomal for x in top_k_cos1])\n",
    "    print(i, top_k_cos2, all_cos2[top_k_cos2], [x in anomal for x in top_k_cos2])\n",
    "    print(i, top_k_cos3, all_cos3[top_k_cos3], [x in anomal for x in top_k_cos3])\n",
    "\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('keras': conda)",
   "language": "python",
   "name": "python37364bitkerascondaa4014ad699b9423aac6666553c6a2934"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}