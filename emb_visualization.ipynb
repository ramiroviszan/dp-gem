{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.models import load_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING: Logging before flag parsing goes to stderr.\nW0428 15:25:40.316967 10880 deprecation_wrapper.py:119] From C:\\Users\\Ramiro\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\n"
    }
   ],
   "source": [
    "model = load_model('results/{exp_name}/deeplog_dp_gen_emb.h5'.format(exp_name=exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 10, 4)             116       \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 40)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 29)                1189      \n=================================================================\nTotal params: 1,305\nTrainable params: 1,305\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "anomal = [6, 7, 9, 11, 12, 13, 14, 18, 23, 26, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6 0 0.26388812 False\n6 1 -0.15393472 False\n6 2 0.5071637 False\n6 3 0.2916785 False\n6 4 0.49635866 False\n6 5 -0.5332525 False\n6 6 1.0 True\n6 7 -0.23415348 True\n6 8 0.23150283 False\n6 9 0.53997374 True\n6 10 0.011769496 False\n6 11 0.14664817 True\n6 12 -0.5076693 True\n6 13 -0.062258214 True\n6 14 -0.4437006 True\n6 15 -0.5963671 False\n6 16 0.20626374 False\n6 17 0.30252993 False\n6 18 0.22264275 True\n6 19 -0.55869514 False\n6 20 -0.90385914 False\n6 21 -0.30932415 False\n6 22 0.41004935 False\n6 23 0.021860689 True\n6 24 0.4929083 False\n6 25 -0.9618191 False\n6 26 -0.8271656 True\n6 27 -0.59472716 False\n6 28 -0.886323 True\n7 0 0.7747382 False\n7 1 0.11835551 False\n7 2 0.0694437 False\n7 3 0.12531567 False\n7 4 -0.8738258 False\n7 5 -0.3194123 False\n7 6 -0.23415348 True\n7 7 1.0000001 True\n7 8 -0.4341766 False\n7 9 0.45068341 True\n7 10 -0.50588113 False\n7 11 -0.4588503 True\n7 12 -0.61634314 True\n7 13 0.6619732 True\n7 14 0.2701052 True\n7 15 -0.29900083 False\n7 16 0.54768354 False\n7 17 -0.77731586 False\n7 18 -0.38240921 True\n7 19 0.92187214 False\n7 20 0.4740469 False\n7 21 -0.3190563 False\n7 22 0.40542632 False\n7 23 0.11058326 True\n7 24 0.3358841 False\n7 25 0.15887187 False\n7 26 0.08709948 True\n7 27 0.64759254 False\n7 28 0.40743196 True\n9 0 0.6599049 False\n9 1 -0.3215605 False\n9 2 0.29727826 False\n9 3 0.14144507 False\n9 4 -0.15332204 False\n9 5 -0.6781033 False\n9 6 0.53997374 True\n9 7 0.45068341 True\n9 8 0.4187677 False\n9 9 1.0 True\n9 10 -0.40463376 False\n9 11 0.28005043 True\n9 12 -0.6019064 True\n9 13 -0.07185374 True\n9 14 0.35320318 True\n9 15 -0.75283444 False\n9 16 0.76236904 False\n9 17 -0.10747951 False\n9 18 0.3497992 True\n9 19 0.26320314 False\n9 20 -0.15907295 False\n9 21 0.10402245 False\n9 22 0.12007092 False\n9 23 -0.33962768 True\n9 24 0.5875026 False\n9 25 -0.42256987 False\n9 26 -0.18819864 True\n9 27 0.28052837 False\n9 28 -0.5508797 True\n11 0 -0.13685887 False\n11 1 -0.8095678 False\n11 2 -0.4914372 False\n11 3 -0.61697423 False\n11 4 0.6318102 False\n11 5 0.26822844 False\n11 6 0.14664817 True\n11 7 -0.4588503 True\n11 8 0.94173414 False\n11 9 0.28005043 True\n11 10 -0.2747537 False\n11 11 0.9999999 True\n11 12 0.5703218 True\n11 13 -0.92934424 True\n11 14 0.63133436 True\n11 15 0.16502704 False\n11 16 0.3937323 False\n11 17 0.81314594 False\n11 18 0.98410904 True\n11 19 -0.29471767 False\n11 20 0.044072002 False\n11 21 0.7427062 False\n11 22 -0.5530679 False\n11 23 -0.90037584 True\n11 24 0.25055802 False\n11 25 0.008318871 False\n11 26 0.30112156 True\n11 27 0.22217837 False\n11 28 -0.58390987 True\n12 0 -0.62582195 False\n12 1 -0.39222312 False\n12 2 -0.7317395 False\n12 3 -0.67071784 False\n12 4 0.48659152 False\n12 5 0.821472 False\n12 6 -0.5076693 True\n12 7 -0.61634314 True\n12 8 0.40411803 False\n12 9 -0.6019064 True\n12 10 0.049011663 False\n12 11 0.5703218 True\n12 12 1.0 True\n12 13 -0.6760798 True\n12 14 0.37445095 True\n12 15 0.8059784 False\n12 16 -0.2579599 False\n12 17 0.6500386 False\n12 18 0.49157286 True\n12 19 -0.27383426 False\n12 20 0.37461546 False\n12 21 0.5967867 False\n12 22 -0.6235892 False\n12 23 -0.49498904 True\n12 24 -0.32267842 False\n12 25 0.5393308 False\n12 26 0.57735527 True\n12 27 0.1374711 False\n12 28 0.13596392 True\n13 0 0.47986382 False\n13 1 0.55250186 False\n13 2 0.33875665 False\n13 3 0.4209485 False\n13 4 -0.6592995 False\n13 5 -0.23396443 False\n13 6 -0.062258214 True\n13 7 0.6619732 True\n13 8 -0.9355557 False\n13 9 -0.07185374 True\n13 10 -0.08706309 False\n13 11 -0.92934424 True\n13 12 -0.6760798 True\n13 13 1.0 True\n13 14 -0.50554276 True\n13 15 -0.16015503 False\n13 16 -0.047379464 False\n13 17 -0.7856082 False\n13 18 -0.8532438 True\n13 19 0.46377677 False\n13 20 0.0031492412 False\n13 21 -0.85320926 False\n13 22 0.7645155 False\n13 23 0.70149803 True\n13 24 0.10743463 False\n13 25 -0.13202375 False\n13 26 -0.399873 True\n13 27 -0.02723828 False\n13 28 0.46922073 True\n14 0 0.17414048 False\n14 1 -0.5811725 False\n14 2 -0.57395387 False\n14 3 -0.56194425 False\n14 4 -0.17998402 False\n14 5 0.20613115 False\n14 6 -0.4437006 True\n14 7 0.2701052 True\n14 8 0.5966487 False\n14 9 0.35320318 True\n14 10 -0.49640185 False\n14 11 0.63133436 True\n14 12 0.37445095 True\n14 13 -0.50554276 True\n14 14 0.9999999 True\n14 15 0.15002069 False\n14 16 0.5927396 False\n14 17 0.13497306 False\n14 18 0.61142194 True\n14 19 0.52583873 False\n14 20 0.73389006 False\n14 21 0.7392428 False\n14 22 -0.55845356 False\n14 23 -0.7875032 True\n14 24 0.15054446 False\n14 25 0.5638007 False\n14 26 0.76406026 True\n14 27 0.8688567 False\n14 28 0.08061498 True\n18 0 0.018127173 False\n18 1 -0.89212775 False\n18 2 -0.5253871 False\n18 3 -0.6731389 False\n18 4 0.63561904 False\n18 5 0.27469945 False\n18 6 0.22264275 True\n18 7 -0.38240921 True\n18 8 0.8945638 False\n18 9 0.3497992 True\n18 10 -0.4157178 False\n18 11 0.98410904 True\n18 12 0.49157286 True\n18 13 -0.8532438 True\n18 14 0.61142194 True\n18 15 0.16147311 False\n18 16 0.5162911 False\n18 17 0.819018 False\n18 18 1.0 True\n18 19 -0.2549574 False\n18 20 0.008644819 False\n18 21 0.62612504 False\n18 22 -0.40124917 False\n18 23 -0.9362186 True\n18 24 0.41675755 False\n18 25 -0.098222435 False\n18 26 0.18961754 True\n18 27 0.24792078 False\n18 28 -0.6463144 True\n23 0 -0.20314085 False\n23 1 0.9460249 False\n23 2 0.71762204 False\n23 3 0.8216605 False\n23 4 -0.39284182 False\n23 5 -0.40107176 False\n23 6 0.021860689 True\n23 7 0.11058326 True\n23 8 -0.75618345 False\n23 9 -0.33962768 True\n23 10 0.6451052 False\n23 11 -0.90037584 True\n23 12 -0.49498904 True\n23 13 0.70149803 True\n23 14 -0.7875032 True\n23 15 -0.2949413 False\n23 16 -0.6749749 False\n23 17 -0.65669274 False\n23 18 -0.9362186 True\n23 19 -0.070582524 False\n23 20 -0.30871463 False\n23 21 -0.5851142 False\n23 22 0.3339189 False\n23 23 1.0 True\n23 24 -0.49795318 False\n23 25 -0.10029894 False\n23 26 -0.34575883 True\n23 27 -0.55158734 False\n23 28 0.4144958 True\n26 0 -0.3497033 False\n26 1 -0.05945635 False\n26 2 -0.44216356 False\n26 3 -0.29525816 False\n26 4 -0.32489103 False\n26 5 0.31552893 False\n26 6 -0.8271656 True\n26 7 0.08709948 True\n26 8 0.31612626 False\n26 9 -0.18819864 True\n26 10 0.035587728 False\n26 11 0.30112156 True\n26 12 0.57735527 True\n26 13 -0.399873 True\n26 14 0.76406026 True\n26 15 0.3347866 False\n26 16 -0.03550154 False\n26 17 -0.07961482 False\n26 18 0.18961754 True\n26 19 0.43906045 False\n26 20 0.85420996 False\n26 21 0.77407414 False\n26 22 -0.79722947 False\n26 23 -0.34575883 True\n26 24 -0.4894269 False\n26 25 0.94083947 False\n26 26 1.0 True\n26 27 0.6629908 False\n26 28 0.55942965 True\n28 0 -0.17259353 False\n28 1 0.53180635 False\n28 2 -0.14752375 False\n28 3 0.08913851 False\n28 4 -0.7193991 False\n28 5 0.2678895 False\n28 6 -0.886323 True\n28 7 0.40743196 True\n28 8 -0.6062891 False\n28 9 -0.5508797 True\n28 10 0.15514448 False\n28 11 -0.58390987 True\n28 12 0.13596392 True\n28 13 0.46922073 True\n28 14 0.08061498 True\n28 15 0.36922258 False\n28 16 -0.36558428 False\n28 17 -0.6478425 False\n28 18 -0.6463144 True\n28 19 0.596334 False\n28 20 0.719823 False\n28 21 -0.059653796 False\n28 22 -0.11597011 False\n28 23 0.4144958 True\n28 24 -0.5545488 False\n28 25 0.7973218 False\n28 26 0.55942965 True\n28 27 0.3813274 False\n28 28 1.0 True\n"
    }
   ],
   "source": [
    "for i in anomal:\n",
    "    for j in range(0, 29):\n",
    "        print(i, j, cosine_similarity(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))[0][0], j in anomal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('results/{exp_name}/deeplog_dp_gen_emb.h5'.format(exp_name='exp_1'))\n",
    "#model2 = load_model('results/{exp_name}/deeplog_dp_gen_emb.h5'.format(exp_name='exp_3'))\n",
    "model3 = load_model('results/{exp_name}/deeplog_dp_gen_emb.h5'.format(exp_name='exp_3'))\n",
    "embedding1 = model1.layers[0].get_weights()[0]\n",
    "embedding2 = model2.layers[0].get_weights()[0]\n",
    "embedding3 = model3.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6 [ 9  2  4 24] [0.53997374 0.5071637  0.49635866 0.4929083 ] [True, False, False, False]\n6 [12 19 27 26] [0.9076451  0.89326286 0.86851096 0.86789733] [True, False, False, True]\n6 [19 14 26  9] [0.8691887  0.86857164 0.843271   0.84148085] [False, True, True, True]\n-----------------\n7 [19  0 13 27] [0.92187214 0.7747382  0.6619732  0.64759254] [False, False, True, False]\n7 [19 27 26  6] [0.88320935 0.88095725 0.8514683  0.8320881 ] [False, False, True, True]\n7 [24 26 27 19] [0.93224996 0.89075387 0.8781583  0.8672353 ] [False, True, False, False]\n-----------------\n9 [16  0 24  6] [0.76236904 0.6599049  0.5875026  0.53997374] [False, False, False, True]\n9 [26 27 19 12] [0.88445497 0.8723365  0.8695332  0.85483104] [True, False, False, True]\n9 [26 27 19 12] [0.96490896 0.95519483 0.9365911  0.9340502 ] [True, False, False, True]\n-----------------\n11 [18  8 17 21] [0.98410904 0.94173414 0.81314594 0.7427062 ] [True, False, False, False]\n11 [19 12  6 26] [0.7687711  0.75770533 0.75708115 0.7440629 ] [False, True, True, True]\n11 [28 20  9 15] [0.86340415 0.86094624 0.82558954 0.82144386] [True, False, True, False]\n-----------------\n12 [ 5 15 17 21] [0.821472  0.8059784 0.6500386 0.5967867] [False, False, False, False]\n12 [26 19 27  6] [0.9573209  0.955487   0.94857556 0.9076451 ] [True, False, False, True]\n12 [27 19 26 14] [0.99550116 0.988664   0.9820833  0.96006525] [False, False, True, True]\n-----------------\n13 [22 23  7  1] [0.7645155  0.70149803 0.6619732  0.55250186] [False, True, True, False]\n13 [ 6 12 19 26] [0.8256214  0.81893724 0.77818596 0.7709583 ] [True, True, False, True]\n13 [20  7 24 27] [0.8072201  0.7928107  0.7837409  0.75964653] [False, True, False, False]\n-----------------\n14 [27 26 21 20] [0.8688567  0.76406026 0.7392428  0.73389006] [False, True, False, False]\n14 [19 27 26 12] [0.91859484 0.916816   0.9075377  0.9040234 ] [False, False, True, True]\n14 [19 26 27 12] [0.98183656 0.9652647  0.9611256  0.96006525] [False, True, False, True]\n-----------------\n18 [11  8 17  4] [0.98410904 0.8945638  0.819018   0.63561904] [True, False, False, False]\n18 [ 3 25  1 22] [0.38379404 0.3310241  0.31953797 0.2826905 ] [False, False, False, False]\n18 [ 2 23 21 13] [0.6258721  0.59442997 0.48804784 0.3464012 ] [False, True, False, True]\n-----------------\n23 [ 1  3  2 13] [0.9460249  0.8216605  0.71762204 0.70149803] [False, False, False, True]\n23 [25  4  2 20] [0.2811337  0.27744097 0.26821032 0.15827505] [False, False, False, False]\n23 [11  9 26 15] [0.73788524 0.6909395  0.63105214 0.61916554] [True, True, True, False]\n-----------------\n26 [25 20 21 14] [0.94083947 0.85420996 0.77407414 0.76406026] [False, False, False, True]\n26 [19 27 12 28] [0.98464286 0.9845277  0.9573209  0.92618847] [False, False, True, True]\n26 [27 19 12 14] [0.9927236 0.988269  0.9820833 0.9652647] [False, False, True, True]\n-----------------\n28 [25 20 19 26] [0.7973218  0.719823   0.596334   0.55942965] [False, False, False, True]\n28 [26 19 27 12] [0.92618847 0.92237675 0.90855455 0.8943893 ] [True, False, False, True]\n28 [14 26 24  9] [0.9518235  0.9405098  0.9345333  0.93152285] [True, True, False, True]\n-----------------\n"
    }
   ],
   "source": [
    "for i in anomal:\n",
    "    all_cos1 = []\n",
    "    all_cos2 = []\n",
    "    all_cos3 = []\n",
    "    for j in range(0, 29):\n",
    "        cos1 = cosine_similarity(embedding1[i].reshape(1, -1), embedding1[j].reshape(1, -1))[0][0]\n",
    "        cos2 = cosine_similarity(embedding2[i].reshape(1, -1), embedding2[j].reshape(1, -1))[0][0]\n",
    "        cos3 = cosine_similarity(embedding3[i].reshape(1, -1), embedding3[j].reshape(1, -1))[0][0]\n",
    "\n",
    "        #print(i, j, cos1, cos2, cos1 - cos2, j in anomal)\n",
    "        all_cos1.append(cos1)\n",
    "        all_cos2.append(cos2)\n",
    "        all_cos3.append(cos3)\n",
    "\n",
    "    all_cos1 = np.array(all_cos1)\n",
    "    all_cos2 = np.array(all_cos2)\n",
    "    all_cos3 = np.array(all_cos3)\n",
    "\n",
    "    top_k_cos1 = np.argsort(all_cos1)[-5:][::-1][1:]\n",
    "    top_k_cos2 = np.argsort(all_cos2)[-5:][::-1][1:]\n",
    "    top_k_cos3 = np.argsort(all_cos3)[-5:][::-1][1:]\n",
    "\n",
    "\n",
    "    print(i, top_k_cos1, all_cos1[top_k_cos1], [x in anomal for x in top_k_cos1])\n",
    "    print(i, top_k_cos2, all_cos2[top_k_cos2], [x in anomal for x in top_k_cos2])\n",
    "    print(i, top_k_cos3, all_cos3[top_k_cos3], [x in anomal for x in top_k_cos3])\n",
    "\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('keras': conda)",
   "language": "python",
   "name": "python37364bitkerascondaa4014ad699b9423aac6666553c6a2934"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}